<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Pytorch-Softmax-Regression"><meta name="keywords" content="SoftmaxRegression"><meta name="author" content="Chenhao"><meta name="copyright" content="Chenhao"><title>Pytorch-Softmax-Regression | Chenhao's Studio</title><link rel="shortcut icon" href="https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/img/C.png"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?55415cdf8caf97abcdf8a6129a90edf2";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-148056531-1', 'auto');
ga('send', 'pageview');</script><link rel="dns-prefetch" href="http://ta.qq.com"><script>(function() {
   var hm = document.createElement("script");
   hm.src = "https://tajs.qq.com/stats?sId=66471139";
   var s = document.getElementsByTagName("script")[0];
   s.parentNode.insertBefore(hm, s);
 })();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#softmax回归从零开始实现"><span class="toc-number">1.</span> <span class="toc-text"> softmax回归从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#获取和读取数据"><span class="toc-number">1.1.</span> <span class="toc-text"> 获取和读取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化模型参数"><span class="toc-number">1.2.</span> <span class="toc-text"> 初始化模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现softmax运算"><span class="toc-number">1.3.</span> <span class="toc-text"> 实现softmax运算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义模型"><span class="toc-number">1.4.</span> <span class="toc-text"> 定义模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义损失函数"><span class="toc-number">1.5.</span> <span class="toc-text"> 定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算分类准确率"><span class="toc-number">1.6.</span> <span class="toc-text"> 计算分类准确率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练模型"><span class="toc-number">1.7.</span> <span class="toc-text"> 训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#预测"><span class="toc-number">1.8.</span> <span class="toc-text"> 预测</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#softmax回归简洁实现"><span class="toc-number">2.</span> <span class="toc-text"> softmax回归简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#获取和读取数据-2"><span class="toc-number">2.1.</span> <span class="toc-text"> 获取和读取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义和初始化模型"><span class="toc-number">2.2.</span> <span class="toc-text"> 定义和初始化模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax和交叉熵损失函数"><span class="toc-number">2.3.</span> <span class="toc-text"> softmax和交叉熵损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义优化算法"><span class="toc-number">2.4.</span> <span class="toc-text"> 定义优化算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练模型-2"><span class="toc-number">2.5.</span> <span class="toc-text"> 训练模型</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/img/readbook.jpg"></div><div class="author-info__name text-center">Chenhao</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/CCChenhao997">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">116</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">100</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">20</span></a></div><!--乌龟--><!--<object type="application/x-shockwave-flash" style="outline:none;" data="http://cdn.abowman.com/widgets/turtles/turtle.swf?" width="260" height="150"><param name="movie" value="http://cdn.abowman.com/widgets/turtles/turtle.swf?"></param><param name="AllowScriptAccess" value="always"></param><param name="wmode" value="opaque"></param><param name="scale" value="noscale"/><param name="salign" value="tl"/></object>--><!--地球--><!--<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5dx5ilw1vqk&amp;s=230&amp;m=0&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=54ff00" async="async"></script>--><!--ClustrMaps--><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=VVjsa_zTGhAUQ1yB5df-C5TI04O6AmJscZXp_OMAvMQ&cl=ffffff&w=a"></script></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/intro/bcar-bg.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Chenhao's Studio</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/slides">Slides</a><a class="site-page" href="/gallery">Gallery</a><a class="site-page" href="/messages">Messages</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">Pytorch-Softmax-Regression</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Pytorch/">Pytorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">1.7k</span><span class="post-meta__separator">|</span><span>阅读时长: 7 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="softmax回归从零开始实现"><a class="markdownIt-Anchor" href="#softmax回归从零开始实现"></a> softmax回归从零开始实现</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"./"</span>)  <span class="comment"># 为了导入上层目录的d21zh_pytorch</span></span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br><span class="line">print(torchvision.__version__)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.1.0</span><br><span class="line">0.3.0</span><br></pre></td></tr></table></figure>
<h2 id="获取和读取数据"><a class="markdownIt-Anchor" href="#获取和读取数据"></a> 获取和读取数据</h2>
<p>我们将使用Fashion-MNIST数据集，并设置批量大小为256。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(len(train_iter))</span><br><span class="line">print(len(test_iter))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">235</span><br><span class="line">40</span><br></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a class="markdownIt-Anchor" href="#初始化模型参数"></a> 初始化模型参数</h2>
<p>跟线性回归中的例子一样，我们将使用向量表示每个样本。已知每个样本输入是高和宽均为28像素的图像。模型的输入向量的长度是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>28</mn><mo>×</mo><mn>28</mn><mo>=</mo><mn>784</mn></mrow><annotation encoding="application/x-tex">28×28=784</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span><span class="mord">8</span><span class="mord">4</span></span></span></span>：该向量的每个元素对应图像中每个像素。由于图像有10个类别，单层神经网络输出层的输出个数为10，因此softmax回归的权重和偏差参数分别为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>784</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">784×10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">7</span><span class="mord">8</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">1×10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span></span></span></span>的矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_outputs)), dtype=torch.float)</span><br><span class="line">b = torch.zeros(num_outputs, dtype=torch.float)</span><br></pre></td></tr></table></figure>
<p>同之前一样，我们要为模型参数附上梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.0104, -0.0037, -0.0224,  ...,  0.0010,  0.0153, -0.0008],</span><br><span class="line">        [-0.0163, -0.0011, -0.0055,  ..., -0.0029, -0.0138,  0.0031],</span><br><span class="line">        [-0.0127,  0.0162, -0.0061,  ...,  0.0093, -0.0120,  0.0107],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.0113,  0.0018,  0.0202,  ...,  0.0008, -0.0034, -0.0122],</span><br><span class="line">        [ 0.0012, -0.0020, -0.0089,  ...,  0.0071, -0.0186, -0.0137],</span><br><span class="line">        [-0.0007,  0.0047, -0.0128,  ...,  0.0095,  0.0062,  0.0013]],</span><br><span class="line">       requires_grad=True)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</span><br></pre></td></tr></table></figure>
<p>在介绍如何定义softmax回归之前，我们先描述一下对如何对多维tensor按维度操作。在下面的例子中，给定一个tensor矩阵X。我们可以只对其中同一列（axis=0）或同一行（axis=1）的元素求和，并在结果中保留行和列这两个维度（keepdims=True）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(X.sum(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>))</span><br><span class="line">print(X.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[5, 7, 9]])</span><br><span class="line">tensor([[ 6],</span><br><span class="line">        [15]])</span><br></pre></td></tr></table></figure>
<h2 id="实现softmax运算"><a class="markdownIt-Anchor" href="#实现softmax运算"></a> 实现softmax运算</h2>
<p>在下面的函数中，矩阵<code>X</code>的行数是样本数，列数是输出个数。为了表达样本预测各个输出的概率，softmax运算会先通过<code>exp</code>函数对每个元素做指数运算，再对<code>exp</code>矩阵同行元素求和，最后令矩阵每行各元素与该行元素之和相除。这样一来，最终得到的矩阵每行元素和为1且非负。因此，该矩阵每行都是合法的概率分布。softmax运算的输出矩阵中的任意一行元素代表了一个样本在各个输出类别上的预测概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span></span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.sum(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># 保持行的维度</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition   <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand((<span class="number">2</span>, <span class="number">5</span>))   <span class="comment"># rand() [0,1]之间均匀分布</span></span><br><span class="line">X_prob = softmax(X)</span><br><span class="line">print(X_prob, X_prob.sum(dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[0.1248, 0.2464, 0.2456, 0.1720, 0.2112],</span><br><span class="line">        [0.2505, 0.1476, 0.2373, 0.1775, 0.1871]]) tensor([1.0000, 1.0000])</span><br></pre></td></tr></table></figure>
<h2 id="定义模型"><a class="markdownIt-Anchor" href="#定义模型"></a> 定义模型</h2>
<p>这里通过<code>view</code>函数将每张原始图像改成长度为<code>num_inputs</code>的向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> softmax(torch.mm(X.view((<span class="number">-1</span>, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数"><a class="markdownIt-Anchor" href="#定义损失函数"></a> 定义损失函数</h2>
<p>为了得到标签的预测概率，我们可以使用<code>gather</code>函数。在下面的例子中，变量<code>y_hat</code>是2个样本在3个类别的预测概率，变量y是这2个样本的标签类别。通过使用<code>gather</code>函数，我们得到了2个样本的标签的预测概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line">y = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat.gather(<span class="number">1</span>, y.view(<span class="number">-1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[0.1000],</span><br><span class="line">        [0.5000]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -torch.log(y_hat.gather(<span class="number">1</span>, y.view(<span class="number">-1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="计算分类准确率"><a class="markdownIt-Anchor" href="#计算分类准确率"></a> 计算分类准确率</h2>
<p>其中<code>y_hat.argmax(dim=1)</code>返回矩阵y_hat每行中最大元素的索引，且返回结果与变量y形状相同。相等条件判断式<code>(y_hat.argmax(dim=1) == y)</code>是一个值为0（相等为假）或1（相等为真）的tensor。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(y_hat, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(dim=<span class="number">1</span>) == y).float().mean().item()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(accuracy(y_hat, y))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.5</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iter, net)</span>:</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).float().sum().item()</span><br><span class="line">        <span class="comment"># print("acc_sum: ",acc_sum)</span></span><br><span class="line">        n += y.shape[<span class="number">0</span>]   <span class="comment"># y.shape[0]表示y的行数</span></span><br><span class="line">        <span class="comment"># print("y.shape[0]: ", y.shape[0])</span></span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(evaluate_accuracy(test_iter, net))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.1117</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a class="markdownIt-Anchor" href="#训练模型"></a> 训练模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 迭代周期数num_epochs和学习率lr</span></span><br><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch3</span><span class="params">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">             params=None, lr=None, optimizer=None)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).sum()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">                    </span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step()   <span class="comment"># "softmax回归的简洁实现"一节将用到</span></span><br><span class="line">                </span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).sum().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        print(<span class="string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">epoch 1, loss 0.7802, train acc 0.751, test acc 0.792</span><br><span class="line">epoch 2, loss 0.5707, train acc 0.812, test acc 0.811</span><br><span class="line">epoch 3, loss 0.5258, train acc 0.826, test acc 0.809</span><br><span class="line">epoch 4, loss 0.5024, train acc 0.831, test acc 0.820</span><br><span class="line">epoch 5, loss 0.4850, train acc 0.838, test acc 0.826</span><br></pre></td></tr></table></figure>
<h2 id="预测"><a class="markdownIt-Anchor" href="#预测"></a> 预测</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = iter(test_iter).next()</span><br><span class="line"></span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.numpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=<span class="number">1</span>).numpy())</span><br><span class="line">titles = [true + <span class="string">'\n'</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> zip(true_labels, pred_labels)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="softmax回归简洁实现"><a class="markdownIt-Anchor" href="#softmax回归简洁实现"></a> softmax回归简洁实现</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>) </span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.1.0</span><br></pre></td></tr></table></figure>
<h2 id="获取和读取数据-2"><a class="markdownIt-Anchor" href="#获取和读取数据-2"></a> 获取和读取数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(len(train_iter))</span><br><span class="line">print(len(test_iter))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">235</span><br><span class="line">40</span><br></pre></td></tr></table></figure>
<h2 id="定义和初始化模型"><a class="markdownIt-Anchor" href="#定义和初始化模型"></a> 定义和初始化模型</h2>
<p>softmax回归的输出层是一个全连接层。因此，我们添加一个输出个数为10的全连接层。我们使用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># class LinearNet(nn.Module):</span></span><br><span class="line"><span class="comment">#     def __init__(self, num_inputs, num_outputs):</span></span><br><span class="line"><span class="comment">#         super(LinearNet, self).__init__()</span></span><br><span class="line"><span class="comment">#         self.linear = nn.Linear(num_inputs, num_outputs)</span></span><br><span class="line"><span class="comment">#     def forward(self, x): # x shape: (batch, 1, 28, 28)</span></span><br><span class="line"><span class="comment">#         y = self.linear(x.view(x.shape[0], -1))</span></span><br><span class="line"><span class="comment">#         return y</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># net = LinearNet(num_inputs, num_outputs)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FlattenLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(FlattenLayer, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>    <span class="comment"># x shape: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># FlattenLayer(),</span></span><br><span class="line">    <span class="comment"># nn.Linear(num_inputs, num_outputs)</span></span><br><span class="line">    OrderedDict(</span><br><span class="line">        [</span><br><span class="line">            (<span class="string">'fatten'</span>, FlattenLayer()), </span><br><span class="line">            (<span class="string">'linear'</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)</span><br></pre></td></tr></table></figure>
<h2 id="softmax和交叉熵损失函数"><a class="markdownIt-Anchor" href="#softmax和交叉熵损失函数"></a> softmax和交叉熵损失函数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h2 id="定义优化算法"><a class="markdownIt-Anchor" href="#定义优化算法"></a> 定义优化算法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练模型-2"><a class="markdownIt-Anchor" href="#训练模型-2"></a> 训练模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">epoch 1, loss 0.0031, train acc 0.749, test acc 0.779</span><br><span class="line">epoch 2, loss 0.0022, train acc 0.814, test acc 0.791</span><br><span class="line">epoch 3, loss 0.0021, train acc 0.826, test acc 0.809</span><br><span class="line">epoch 4, loss 0.0020, train acc 0.832, test acc 0.812</span><br><span class="line">epoch 5, loss 0.0019, train acc 0.837, test acc 0.829</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Chenhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://chenhao.space/post/eade7da1.html">http://chenhao.space/post/eade7da1.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://chenhao.space">Chenhao's Studio</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SoftmaxRegression/">SoftmaxRegression</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/post/fc895aa3.html"><i class="fa fa-chevron-left">  </i><span>Pytorch-MLP</span></a></div><div class="next-post pull-right"><a href="/post/d5e92e8c.html"><span>Pytorch-Fashion-MNIST</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27867140&auto=0&height=66"></iframe></div><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'GjEQNoax0r5e528SPYqxOPME-gzGzoHsz',
  appKey:'vLtYMpgjD90htoYal5mIjk4b',
  placeholder:'Just go go',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/intro/bcar-bg.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By Chenhao</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">🍉🍉🍉 Hi, welcome to my <a href="http://chenhao.space">blog</a>! 🍉🍉🍉</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/love.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>