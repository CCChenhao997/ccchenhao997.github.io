<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Pytorch-Linear_Regression"><meta name="keywords" content="LinearRegression"><meta name="author" content="Chenhao"><meta name="copyright" content="Chenhao"><title>Pytorch-Linear_Regression | Chenhao's Studio</title><link rel="shortcut icon" href="https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/img/C.png"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?55415cdf8caf97abcdf8a6129a90edf2";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-148056531-1', 'auto');
ga('send', 'pageview');</script><link rel="dns-prefetch" href="http://ta.qq.com"><script>(function() {
   var hm = document.createElement("script");
   hm.src = "https://tajs.qq.com/stats?sId=66471139";
   var s = document.getElementsByTagName("script")[0];
   s.parentNode.insertBefore(hm, s);
 })();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#线性回归"><span class="toc-number">1.</span> <span class="toc-text"> 线性回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#线性回归从零开始实现"><span class="toc-number">2.</span> <span class="toc-text"> 线性回归从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#生成数据集"><span class="toc-number">2.1.</span> <span class="toc-text"> 生成数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#读取数据"><span class="toc-number">2.2.</span> <span class="toc-text"> 读取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化模型参数"><span class="toc-number">2.3.</span> <span class="toc-text"> 初始化模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义模型"><span class="toc-number">2.4.</span> <span class="toc-text"> 定义模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义损失函数"><span class="toc-number">2.5.</span> <span class="toc-text"> 定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义优化算法"><span class="toc-number">2.6.</span> <span class="toc-text"> 定义优化算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练模型"><span class="toc-number">2.7.</span> <span class="toc-text"> 训练模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#线性回归的简洁实现"><span class="toc-number">3.</span> <span class="toc-text"> 线性回归的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#生成数据集-2"><span class="toc-number">3.1.</span> <span class="toc-text"> 生成数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#读取数据-2"><span class="toc-number">3.2.</span> <span class="toc-text"> 读取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义模型-2"><span class="toc-number">3.3.</span> <span class="toc-text"> 定义模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化模型参数-2"><span class="toc-number">3.4.</span> <span class="toc-text"> 初始化模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义损失函数-2"><span class="toc-number">3.5.</span> <span class="toc-text"> 定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义优化算法-2"><span class="toc-number">3.6.</span> <span class="toc-text"> 定义优化算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练模型-2"><span class="toc-number">3.7.</span> <span class="toc-text"> 训练模型</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/img/readbook.jpg"></div><div class="author-info__name text-center">Chenhao</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/CCChenhao997">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">43</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">11</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://zyu963.github.io/index.html">ZY_FZU</a><a class="author-info-links__name text-center" href="https://f0rbeta.github.io/">BJH_NJUPT</a></div><!--乌龟--><object type="application/x-shockwave-flash" style="outline:none;" data="http://cdn.abowman.com/widgets/turtles/turtle.swf?" width="260" height="150"><param name="movie" value="http://cdn.abowman.com/widgets/turtles/turtle.swf?"><param name="AllowScriptAccess" value="always"><param name="wmode" value="opaque"><param name="scale" value="noscale"><param name="salign" value="tl"></object><!--地球--><!--<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5dx5ilw1vqk&amp;s=230&amp;m=0&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=54ff00" async="async"></script>--><!--ClustrMaps--><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=VVjsa_zTGhAUQ1yB5df-C5TI04O6AmJscZXp_OMAvMQ&cl=ffffff&w=a"></script></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/intro/bcar-bg.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Chenhao's Studio</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/slides">Slides</a><a class="site-page" href="/gallery">Gallery</a><a class="site-page" href="/messages">Messages</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">Pytorch-Linear_Regression</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Pytorch/">Pytorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">3.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 14 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="线性回归"><a class="markdownIt-Anchor" href="#线性回归"></a> 线性回归</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.1.0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">1000</span>)</span><br><span class="line">b = torch.ones(<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>将这两个向量按元素逐一做标量加法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">start = time()</span><br><span class="line">c = torch.zeros(<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    c[i] = a[i] + b[i]</span><br><span class="line">print(time() - start)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.014781951904296875</span><br></pre></td></tr></table></figure>
<p>将这两个向量直接做矢量加法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">start = time()</span><br><span class="line">d = a + b</span><br><span class="line">print(time() - start)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.0003459453582763672</span><br></pre></td></tr></table></figure>
<p><strong>结果很明显，后者比前者更省时。因此，我们应该尽可能采用矢量计算，以提升计算效率。</strong></p>
<p>广播机制例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>)</span><br><span class="line">b = <span class="number">10</span></span><br><span class="line">print(a + b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([11., 11., 11.])</span><br></pre></td></tr></table></figure>
<h1 id="线性回归从零开始实现"><a class="markdownIt-Anchor" href="#线性回归从零开始实现"></a> 线性回归从零开始实现</h1>
<p>设房屋的面积为 x1 ，房龄为 x2 ，售出价格为 y 。我们需要建立基于输入 x1 和 x2 来计算输出 y 的表达式，也就是模型（model）。顾名思义，线性回归假设输出与各个输入之间是线性关系：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub><msub><mi>w</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">ŷ =x_1w_1+x_2w_2+b,
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">b</span><span class="mpunct">,</span></span></span></span></span></p>
<p>其中 w1 和 w2 是权重（weight）， b 是偏差（bias），且均为标量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br><span class="line">torch.set_default_tensor_type(<span class="string">'torch.FloatTensor'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.1.0</span><br></pre></td></tr></table></figure>
<h2 id="生成数据集"><a class="markdownIt-Anchor" href="#生成数据集"></a> 生成数据集</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.randn(num_examples, num_inputs)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)</span></span><br><span class="line"><span class="comment"># 是增加的随机噪声，噪声代表了数据集中无意义的干扰</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意，features的每一行是一个长度为2的向量，而labels的每一行是一个长度为1的向量（标量）。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(features[<span class="number">0</span>], labels[<span class="number">0</span>])</span><br><span class="line">print(labels.size())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([1.0796, 0.3098]) tensor(5.3111)</span><br><span class="line">torch.Size([1000])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_svg_display</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 用矢量图显示</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">'svg'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_figsize</span><span class="params">(figsize=<span class="params">(<span class="number">3.5</span>, <span class="number">2.5</span>)</span>)</span>:</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置图的尺寸</span></span><br><span class="line">    plt.rcParams[<span class="string">'figure.figsize'</span>] = figsize</span><br><span class="line">    </span><br><span class="line"><span class="comment"># # 在../d2lzh_pytorch里面添加上面两个函数后就可以这样导入</span></span><br><span class="line"><span class="comment"># import sys</span></span><br><span class="line"><span class="comment"># sys.path.append("..")</span></span><br><span class="line"><span class="comment"># from d2lzh_pytorch import * </span></span><br><span class="line"></span><br><span class="line">set_figsize()</span><br><span class="line">plt.scatter(features[:, <span class="number">1</span>].numpy(), labels.numpy(), <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p><img src="https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/PytorchLearning/output_16_0.svg" alt></p>
<h2 id="读取数据"><a class="markdownIt-Anchor" href="#读取数据"></a> 读取数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。</span></span><br><span class="line"><span class="string">这里我们定义一个函数：它每次返回batch_size（批量大小）个随机样本的特征和标签。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d21zh中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span><span class="params">(batch_size, features, labels)</span>:</span></span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)  <span class="comment"># 样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        <span class="comment"># 最后一次可能不足一个batch</span></span><br><span class="line">        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure>
<p>让我们读取第一个小批量数据样本并打印。每个批量的特征形状为(10, 2)，分别对应批量大小和输入个数；标签形状为批量大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.0758, -0.3826],</span><br><span class="line">        [-0.0732, -2.8249],</span><br><span class="line">        [-0.7557,  0.7309],</span><br><span class="line">        [-0.6299, -0.1189],</span><br><span class="line">        [-0.9627,  0.0947],</span><br><span class="line">        [ 1.3580,  1.2370],</span><br><span class="line">        [-0.3616,  0.3718],</span><br><span class="line">        [-0.5052, -1.4169],</span><br><span class="line">        [-1.4944, -1.5984],</span><br><span class="line">        [-0.2773, -0.4932]]) </span><br><span class="line"> tensor([ 5.6482, 13.6693,  0.2113,  3.3605,  1.9548,  2.7224,  2.2263,  7.9824,</span><br><span class="line">         6.6399,  5.3169])</span><br></pre></td></tr></table></figure>
<h2 id="初始化模型参数"><a class="markdownIt-Anchor" href="#初始化模型参数"></a> 初始化模型参数</h2>
<p>我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此我们需要创建它们的梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([0.], requires_grad=True)</span><br></pre></td></tr></table></figure>
<h2 id="定义模型"><a class="markdownIt-Anchor" href="#定义模型"></a> 定义模型</h2>
<p>使用<code>mm()</code>函数做矩阵乘法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在d21zh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span><span class="params">(X, w, b)</span>:</span>   </span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数"><a class="markdownIt-Anchor" href="#定义损失函数"></a> 定义损失函数</h2>
<p>我们需要把真实值 <code>y</code> 变形成预测值 <code>y_hat</code> 的形状。以下函数返回的结果也将和 <code>y_hat</code> 的形状相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在pytorch_d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span><span class="params">(y_hat, y)</span>:</span>  </span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> /<span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="定义优化算法"><a class="markdownIt-Anchor" href="#定义优化算法"></a> 定义优化算法</h2>
<p>以下的<code>sgd</code>函数实现了小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在pytorch_d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        <span class="comment"># 注意这里更改param时用的param.data</span></span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a class="markdownIt-Anchor" href="#训练模型"></a> 训练模型</h2>
<p>在训练中，我们将多次迭代模型参数。在每次迭代中，我们根据当前读取的小批量数据样本（特征<code>X</code>和标签<code>y</code>），通过调用反向函数<code>backward</code>计算小批量随机梯度，并调用优化算法<code>sgd</code>迭代模型参数。由于我们之前设批量大小<code>batch_size</code>为10，每个小批量的损失<code>l</code>的形状为(10, 1)。回忆一下“自动求梯度”一节。由于变量<code>l</code>并不是一个标量，运行<code>l.backward()</code>将对<code>l</code>中元素求和得到新的变量，再求该变量有关模型参数的梯度。</p>
<p>在一个迭代周期（epoch）中，我们将完整遍历一遍<code>data_iter</code>函数，并对训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是超参数，分别设3和0.03。在实践中，大多超参数都需要通过反复试错来不断调节。虽然迭代周期数设得越大模型可能越有效，但是训练时间可能过长。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs): </span><br><span class="line">    <span class="comment"># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。</span></span><br><span class="line">    <span class="comment"># X和y分别是小批量样本的特征和标签</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y).sum()  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()   <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 不要忘了梯度清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">        </span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    print(<span class="string">'epoch %d, loss %f'</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">epoch 1, loss 0.000051</span><br><span class="line">epoch 2, loss 0.000051</span><br><span class="line">epoch 3, loss 0.000050</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(true_w, <span class="string">'\n'</span>, w)</span><br><span class="line">print(true_b, <span class="string">'\n'</span>, b)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2, -3.4] </span><br><span class="line"> tensor([[ 1.9999],</span><br><span class="line">        [-3.3999]], requires_grad=True)</span><br><span class="line">4.2 </span><br><span class="line"> tensor([4.2008], requires_grad=True)</span><br></pre></td></tr></table></figure>
<h1 id="线性回归的简洁实现"><a class="markdownIt-Anchor" href="#线性回归的简洁实现"></a> 线性回归的简洁实现</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br><span class="line">torch.set_default_tensor_type(<span class="string">'torch.FloatTensor'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.1.0</span><br></pre></td></tr></table></figure>
<h2 id="生成数据集-2"><a class="markdownIt-Anchor" href="#生成数据集-2"></a> 生成数据集</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">100</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, num_inputs)), dtype=torch.float)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加上随机噪声</span></span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.float)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.1243, -0.5666],</span><br><span class="line">        [ 1.7075, -1.6025],</span><br><span class="line">        [ 0.8301,  0.2019],</span><br><span class="line">        [ 1.0534,  0.1100],</span><br><span class="line">        [ 2.4399, -0.4741],</span><br><span class="line">        [-0.0539,  0.0243],</span><br><span class="line">        [ 1.4430, -1.4239],</span><br><span class="line">        [-0.6309, -0.6679],</span><br><span class="line">        [-0.2309, -0.5457],</span><br><span class="line">        [ 1.6393,  1.2493],</span><br><span class="line">        [-0.0947,  0.1160],</span><br><span class="line">        [ 0.4428, -0.8794],</span><br><span class="line">        [-1.5046,  1.1385],</span><br><span class="line">        [ 1.3436, -0.7787],</span><br><span class="line">        [-1.7918, -1.1573],</span><br><span class="line">        [-1.5081, -0.3098],</span><br><span class="line">        [ 1.1218,  0.2899],</span><br><span class="line">        [ 0.1286, -0.2290],</span><br><span class="line">        [-0.1047,  0.5363],</span><br><span class="line">        [-0.2209, -0.3525],</span><br><span class="line">        [ 0.1808, -0.5736],</span><br><span class="line">        [-1.2324,  0.2134],</span><br><span class="line">        [ 0.3638,  0.9431],</span><br><span class="line">        [-2.2382, -1.2264],</span><br><span class="line">        [-0.3424, -1.4127],</span><br><span class="line">        [-0.6259, -0.5414],</span><br><span class="line">        [ 1.5641, -1.0523],</span><br><span class="line">        [-0.1465,  1.9687],</span><br><span class="line">        [-2.0239,  1.3050],</span><br><span class="line">        [ 0.6236,  0.9445],</span><br><span class="line">        [-1.8673, -0.2439],</span><br><span class="line">        [ 1.0122, -0.5010],</span><br><span class="line">        [ 0.7602, -0.6323],</span><br><span class="line">        [-1.1825, -0.4863],</span><br><span class="line">        [ 0.4238, -1.1245],</span><br><span class="line">        [ 0.6046,  0.0247],</span><br><span class="line">        [-0.0610,  0.0560],</span><br><span class="line">        [ 0.8459,  1.9671],</span><br><span class="line">        [-1.5314,  1.9764],</span><br><span class="line">        [-0.0863,  0.3367],</span><br><span class="line">        [ 0.4751, -1.4216],</span><br><span class="line">        [-2.3350, -1.5692],</span><br><span class="line">        [-0.0806,  1.0371],</span><br><span class="line">        [ 2.2874, -0.2344],</span><br><span class="line">        [ 0.7814,  0.5793],</span><br><span class="line">        [ 0.3327, -0.3436],</span><br><span class="line">        [ 1.1383, -1.5865],</span><br><span class="line">        [ 0.3443, -1.1161],</span><br><span class="line">        [-0.1329, -0.5532],</span><br><span class="line">        [ 0.4722, -1.0481],</span><br><span class="line">        [-0.1027,  0.3339],</span><br><span class="line">        [ 1.7872, -0.5495],</span><br><span class="line">        [ 0.5177, -0.9145],</span><br><span class="line">        [-0.7556, -0.3341],</span><br><span class="line">        [ 1.3349,  0.1399],</span><br><span class="line">        [-0.3999,  0.6756],</span><br><span class="line">        [ 0.7278, -0.0186],</span><br><span class="line">        [ 0.1343, -2.8902],</span><br><span class="line">        [ 1.3208, -0.0030],</span><br><span class="line">        [ 0.7743, -0.4380],</span><br><span class="line">        [-1.4138, -0.1755],</span><br><span class="line">        [-0.2573, -1.5179],</span><br><span class="line">        [ 1.5327, -0.1858],</span><br><span class="line">        [ 0.0104, -1.1359],</span><br><span class="line">        [-0.8267,  0.2327],</span><br><span class="line">        [-0.3347,  1.2213],</span><br><span class="line">        [-0.2930,  1.4084],</span><br><span class="line">        [ 0.0039, -0.7128],</span><br><span class="line">        [ 0.2927,  1.3857],</span><br><span class="line">        [-1.3342,  1.7952],</span><br><span class="line">        [ 1.3093,  0.1347],</span><br><span class="line">        [ 0.6866, -1.5838],</span><br><span class="line">        [ 0.2249,  1.3329],</span><br><span class="line">        [-2.1616, -0.8585],</span><br><span class="line">        [-0.1465, -0.4043],</span><br><span class="line">        [ 1.3356, -0.3467],</span><br><span class="line">        [ 0.1955,  0.6419],</span><br><span class="line">        [ 0.6358,  2.5126],</span><br><span class="line">        [-1.9304,  0.1323],</span><br><span class="line">        [ 0.1517,  0.7876],</span><br><span class="line">        [-1.2069,  1.0423],</span><br><span class="line">        [ 0.1724,  0.2650],</span><br><span class="line">        [-1.1125, -0.3688],</span><br><span class="line">        [-0.5661, -0.1372],</span><br><span class="line">        [ 0.5544, -1.0634],</span><br><span class="line">        [ 0.3861, -0.2862],</span><br><span class="line">        [-2.0400, -0.1196],</span><br><span class="line">        [-0.2448,  0.6680],</span><br><span class="line">        [ 0.6154,  1.1794],</span><br><span class="line">        [ 0.4105, -0.6895],</span><br><span class="line">        [ 1.8495,  0.4164],</span><br><span class="line">        [ 0.0693, -1.4827],</span><br><span class="line">        [ 0.2276, -0.8581],</span><br><span class="line">        [-0.1439, -0.1953],</span><br><span class="line">        [ 0.6906, -1.6262],</span><br><span class="line">        [-0.3355,  1.4109],</span><br><span class="line">        [ 0.1661, -1.3578],</span><br><span class="line">        [ 0.1400,  0.2831],</span><br><span class="line">        [-2.2244, -0.0052],</span><br><span class="line">        [ 0.1870,  0.6322]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([ 6.3582, 13.0468,  5.1703,  5.9370, 10.6816,  4.0108, 11.9330,  5.2226,</span><br><span class="line">         5.5883,  3.2406,  3.6132,  8.0711, -2.6950,  9.5377,  4.5423,  2.2213,</span><br><span class="line">         5.4405,  5.2333,  2.1839,  4.9762,  6.5070,  0.9976,  1.7188,  3.8831,</span><br><span class="line">         8.3345,  4.7859, 10.8829, -2.7822, -4.2824,  2.2112,  1.3012,  7.9229,</span><br><span class="line">         7.8523,  3.5080,  8.8797,  5.3386,  3.8763, -0.7937, -5.6060,  2.8852,</span><br><span class="line">         9.9732,  4.8737,  0.5205,  9.5834,  3.8199,  6.0169, 11.8725,  8.6902,</span><br><span class="line">         5.8258,  8.7158,  2.8549,  9.6557,  8.3736,  3.8157,  6.3899,  1.1137,</span><br><span class="line">         5.7034, 14.2933,  6.8431,  7.2523,  1.9778,  8.8505,  7.8973,  8.0876,</span><br><span class="line">         1.7574, -0.6365, -1.1802,  6.6373,  0.0778, -4.5783,  6.3599, 10.9425,</span><br><span class="line">         0.1049,  2.7995,  5.2798,  8.0636,  2.4182, -3.0564, -0.1136,  1.8242,</span><br><span class="line">        -1.7579,  3.6434,  3.2396,  3.5254,  8.9399,  5.9625,  0.5436,  1.4391,</span><br><span class="line">         1.4254,  7.3642,  6.4810,  9.3788,  7.5766,  4.5753, 11.1097, -1.2708,</span><br><span class="line">         9.1500,  3.5305, -0.2608,  2.4223])</span><br></pre></td></tr></table></figure>
<h2 id="读取数据-2"><a class="markdownIt-Anchor" href="#读取数据-2"></a> 读取数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line"><span class="comment"># 将数据集转成 torch 能识别的 dataset</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"><span class="comment"># print(dict(dataset))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 dataset 放入 DataLoader，可以生成一个迭代器，从而我们可以方便的进行批处理。</span></span><br><span class="line">data_iter = Data.DataLoader(</span><br><span class="line">    dataset = dataset,         <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size = batch_size,   <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle = <span class="literal">False</span>,            <span class="comment"># 要不要打乱数据（打乱比较好）</span></span><br><span class="line">    num_workers = <span class="number">2</span>,           <span class="comment"># 多线程来读数据</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>dataset：Dataset类型，从其中加载数据</li>
<li>batch_size：int，可选。每个batch加载多少样本</li>
<li>shuffle：bool，可选。为True时表示每个epoch都对数据进行洗牌</li>
<li>sampler：Sampler，可选。从数据集中采样样本的方法。</li>
<li>num_workers：int，可选。加载数据时使用多少子进程。默认值为0，表示在主进程中加载数据。</li>
<li>collate_fn：callable，可选。</li>
<li>pin_memory：bool，可选。</li>
<li>drop_last：bool，可选。True表示如果最后剩下不完全的batch,丢弃。False表示不丢弃。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    print(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.1243, -0.5666],</span><br><span class="line">        [ 1.7075, -1.6025],</span><br><span class="line">        [ 0.8301,  0.2019],</span><br><span class="line">        [ 1.0534,  0.1100],</span><br><span class="line">        [ 2.4399, -0.4741],</span><br><span class="line">        [-0.0539,  0.0243],</span><br><span class="line">        [ 1.4430, -1.4239],</span><br><span class="line">        [-0.6309, -0.6679],</span><br><span class="line">        [-0.2309, -0.5457],</span><br><span class="line">        [ 1.6393,  1.2493]]) </span><br><span class="line"> tensor([ 6.3582, 13.0468,  5.1703,  5.9370, 10.6816,  4.0108, 11.9330,  5.2226,</span><br><span class="line">         5.5883,  3.2406])</span><br></pre></td></tr></table></figure>
<h2 id="定义模型-2"><a class="markdownIt-Anchor" href="#定义模型-2"></a> 定义模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature)</span>:</span></span><br><span class="line">        super(LinearNet, self).__init__()</span><br><span class="line">        self.LinearNet = nn.Linear(n_feature, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line">print(net)     <span class="comment"># 打印出网络结构</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LinearNet(</span><br><span class="line">  (LinearNet): Linear(in_features=2, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>Sequential</code>实例可以看作是一个串联各个层的容器。在构造模型时，我们在该容器中依次添加层。当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写法一</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)   <span class="comment"># nn.Linear(input_size, output_size)</span></span><br><span class="line">    <span class="comment"># 此处还可以传入其他层</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">'linear'</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># net.add_module</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法三</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    OrderedDict(</span><br><span class="line">        [</span><br><span class="line">            (<span class="string">'linear'</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># ......</span></span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(net)</span><br><span class="line">print(net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (linear): Linear(in_features=2, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line">Linear(in_features=2, out_features=1, bias=True)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(list(net.parameters()))    <span class="comment"># weight 和 bias</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Parameter containing:</span><br><span class="line">tensor([[0.1465, 0.2274]], requires_grad=True), Parameter containing:</span><br><span class="line">tensor([0.5282], requires_grad=True)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[0.2274, 0.5282]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.6705], requires_grad=True)</span><br></pre></td></tr></table></figure>
<h2 id="初始化模型参数-2"><a class="markdownIt-Anchor" href="#初始化模型参数-2"></a> 初始化模型参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0.0</span>)  </span><br><span class="line"><span class="comment"># 也可以直接修改bias的data: net[0].bias.data.fill_(0)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([0.], requires_grad=True)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    print(param)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[ 0.0031, -0.0114]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.], requires_grad=True)</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数-2"><a class="markdownIt-Anchor" href="#定义损失函数-2"></a> 定义损失函数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure>
<h2 id="定义优化算法-2"><a class="markdownIt-Anchor" href="#定义优化算法-2"></a> 定义优化算法</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SGD (</span><br><span class="line">Parameter Group 0</span><br><span class="line">    dampening: 0</span><br><span class="line">    lr: 0.03</span><br><span class="line">    momentum: 0</span><br><span class="line">    nesterov: False</span><br><span class="line">    weight_decay: 0</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为不同子网络设置不同的学习率</span></span><br><span class="line"><span class="comment"># optimizer =optim.SGD([</span></span><br><span class="line"><span class="comment">#                 # 如果对某个参数不指定学习率，就使用最外层的默认学习率</span></span><br><span class="line"><span class="comment">#                 &#123;'params': net.subnet1.parameters()&#125;, # lr=0.03</span></span><br><span class="line"><span class="comment">#                 &#123;'params': net.subnet2.parameters(), 'lr': 0.01&#125;</span></span><br><span class="line"><span class="comment">#             ], lr=0.03)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # 调整学习率</span></span><br><span class="line"><span class="comment"># for param_group in optimizer.param_groups:</span></span><br><span class="line"><span class="comment">#     param_group['lr'] *= 0.1 # 学习率为之前的0.1倍</span></span><br></pre></td></tr></table></figure>
<h2 id="训练模型-2"><a class="markdownIt-Anchor" href="#训练模型-2"></a> 训练模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = net(X)   <span class="comment"># X是数据(特征向量)，y是标签(scalar)</span></span><br><span class="line">        l = loss(output, y.view(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    print(<span class="string">'epoch %d, loss: %f'</span> % (epoch, l.item()))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">epoch 1, loss: 11.934128</span><br><span class="line">epoch 2, loss: 2.813488</span><br><span class="line">epoch 3, loss: 0.671115</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dense = net[<span class="number">0</span>]</span><br><span class="line">print(true_w, dense.weight.data)</span><br><span class="line">print(true_b, dense.bias.data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2, -3.4] tensor([[ 1.8862, -3.0119]])</span><br><span class="line">4.2 tensor([3.6755])</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Chenhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://chenhao.space/post/7f30013.html">http://chenhao.space/post/7f30013.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://chenhao.space">Chenhao's Studio</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LinearRegression/">LinearRegression</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="next-post pull-right"><a href="/post/38c7e284.html"><span>口语学习13-表达和回应赞美</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27867140&auto=0&height=66"></iframe></div><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'GjEQNoax0r5e528SPYqxOPME-gzGzoHsz',
  appKey:'vLtYMpgjD90htoYal5mIjk4b',
  placeholder:'Just go go',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/intro/bcar-bg.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By Chenhao</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">🍉🍉🍉 Hi, welcome to my <a href="http://chenhao.space">blog</a>! 🍉🍉🍉</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/love.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>