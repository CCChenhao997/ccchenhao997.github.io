<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Pytorch-IMDB电影评论情感分析"><meta name="keywords" content="SA"><meta name="author" content="Chenhao"><meta name="copyright" content="Chenhao"><title>Pytorch-IMDB电影评论情感分析 | Chenhao's Studio</title><link rel="shortcut icon" href="https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/img/C.png"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?55415cdf8caf97abcdf8a6129a90edf2";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-148056531-1', 'auto');
ga('send', 'pageview');</script><link rel="dns-prefetch" href="http://ta.qq.com"><script>(function() {
   var hm = document.createElement("script");
   hm.src = "https://tajs.qq.com/stats?sId=66471139";
   var s = document.getElementsByTagName("script")[0];
   s.parentNode.insertBefore(hm, s);
 })();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-情感分析"><span class="toc-number">1.</span> <span class="toc-text"> Pytorch-情感分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#第一步导入imdb电影数据集只有训练集和测试集"><span class="toc-number">1.1.</span> <span class="toc-text"> 第一步：导入IMDB电影数据集，只有训练集和测试集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第二步将训练集划分为训练集和验证集"><span class="toc-number">1.2.</span> <span class="toc-text"> 第二步：将训练集划分为训练集和验证集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第三步用训练集建立vocabulary把每个单词映射到一个数字"><span class="toc-number">1.3.</span> <span class="toc-text"> 第三步：用训练集建立vocabulary，把每个单词映射到一个数字。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第四步创建iterators每个iteration都会返回一个batch的样本"><span class="toc-number">1.4.</span> <span class="toc-text"> 第四步：创建iterators，每个iteration都会返回一个batch的样本。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第五步创建-word-averaging-模型"><span class="toc-number">1.5.</span> <span class="toc-text"> 第五步：创建 Word Averaging 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第六步初始化参数"><span class="toc-number">1.6.</span> <span class="toc-text"> 第六步：初始化参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第七步训练模型"><span class="toc-number">1.7.</span> <span class="toc-text"> 第七步：训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第八步查看模型运行结果"><span class="toc-number">1.8.</span> <span class="toc-text"> 第八步：查看模型运行结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第九步预测结果"><span class="toc-number">1.9.</span> <span class="toc-text"> 第九步：预测结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnn模型bilstm"><span class="toc-number">1.10.</span> <span class="toc-text"> RNN模型（BiLSTM）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化参数"><span class="toc-number">1.11.</span> <span class="toc-text"> 初始化参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练rnn模型"><span class="toc-number">1.12.</span> <span class="toc-text"> 训练RNN模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#预测结果"><span class="toc-number">1.13.</span> <span class="toc-text"> 预测结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cnn模型"><span class="toc-number">1.14.</span> <span class="toc-text"> CNN模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-number">1.15.</span> <span class="toc-text"> 参考资料</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/img/readbook.jpg"></div><div class="author-info__name text-center">Chenhao</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/CCChenhao997">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">57</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">51</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">11</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://zyu963.github.io/index.html">ZY_FZU</a><a class="author-info-links__name text-center" href="https://f0rbeta.github.io/">BJH_NJUPT</a></div><!--乌龟--><object type="application/x-shockwave-flash" style="outline:none;" data="http://cdn.abowman.com/widgets/turtles/turtle.swf?" width="260" height="150"><param name="movie" value="http://cdn.abowman.com/widgets/turtles/turtle.swf?"><param name="AllowScriptAccess" value="always"><param name="wmode" value="opaque"><param name="scale" value="noscale"><param name="salign" value="tl"></object><!--地球--><!--<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5dx5ilw1vqk&amp;s=230&amp;m=0&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=54ff00" async="async"></script>--><!--ClustrMaps--><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=VVjsa_zTGhAUQ1yB5df-C5TI04O6AmJscZXp_OMAvMQ&cl=ffffff&w=a"></script></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/intro/bcar-bg.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Chenhao's Studio</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/slides">Slides</a><a class="site-page" href="/gallery">Gallery</a><a class="site-page" href="/messages">Messages</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">Pytorch-IMDB电影评论情感分析</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Pytorch/">Pytorch</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">6.6k</span><span class="post-meta__separator">|</span><span>阅读时长: 35 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="pytorch-情感分析"><a class="markdownIt-Anchor" href="#pytorch-情感分析"></a> Pytorch-情感分析</h1>
<h2 id="第一步导入imdb电影数据集只有训练集和测试集"><a class="markdownIt-Anchor" href="#第一步导入imdb电影数据集只有训练集和测试集"></a> 第一步：导入IMDB电影数据集，只有训练集和测试集</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED)  <span class="comment"># 为CPU设置随机种子</span></span><br><span class="line">torch.cuda.manual_seed(SEED)  <span class="comment">#为GPU设置随机种子</span></span><br><span class="line"><span class="comment"># 在程序刚开始加这条语句可以提升一点训练速度，没什么额外开销</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先，我们要创建两个Field 对象：这两个对象包含了我们打算如何预处理文本数据的信息。</span></span><br><span class="line"><span class="comment"># spaCy:英语分词器,类似于NLTK库，如果没有传递tokenize参数，则默认只是在空格上拆分字符串。</span></span><br><span class="line"><span class="comment"># torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）</span></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)</span><br><span class="line"><span class="comment">#LabelField是Field类的一个特殊子集，专门用于处理标签。 </span></span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载IMDB电影评论数据集</span></span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">downloading aclImdb_v1.tar.gz</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07&lt;00:00, 11.1MB/s]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看数据集</span></span><br><span class="line">print(vars(train_data.examples[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos;text&apos;: [&apos;This&apos;, &apos;is&apos;, &apos;one&apos;, &apos;of&apos;, &apos;the&apos;, &apos;finest&apos;, &apos;films&apos;, &apos;to&apos;, &apos;come&apos;, &apos;out&apos;, &apos;of&apos;, &apos;Hong&apos;, &apos;Kong&apos;, &quot;&apos;s&quot;, &quot;&apos;&quot;, &apos;New&apos;, &apos;Wave&apos;, &quot;&apos;&quot;, &apos;that&apos;, &apos;began&apos;, &apos;with&apos;, &apos;Tsui&apos;, &apos;Hark&apos;, &quot;&apos;s&quot;, &apos;&quot;&apos;, &apos;ZU&apos;, &apos;:&apos;, &apos;Warriors&apos;, &apos;of&apos;, &apos;Magic&apos;, &apos;Mountain&apos;, &apos;&quot;&apos;, &apos;.&apos;, &apos;Tsui&apos;, &apos;set&apos;, &apos;a&apos;, &apos;tone&apos;, &apos;for&apos;, &apos;the&apos;, &apos;New&apos;, &apos;Wave&apos;, &quot;&apos;s&quot;, &apos;approach&apos;, &apos;to&apos;, &apos;the&apos;, &apos;martial&apos;, &apos;arts&apos;, &apos;film&apos;, &apos;that&apos;, &apos;pretty&apos;, &apos;much&apos;, &apos;all&apos;, &apos;the&apos;, &apos;directors&apos;, &apos;of&apos;, &apos;the&apos;, &apos;New&apos;, &apos;Wave&apos;, &apos;(&apos;, &apos;Jackie&apos;, &apos;Chan&apos;, &apos;,&apos;, &apos;Sammo&apos;, &apos;Hung&apos;, &apos;,&apos;, &apos;Wong&apos;, &apos;Jing&apos;, &apos;,&apos;, &apos;Ching&apos;, &apos;Siu&apos;, &apos;Tung&apos;, &apos;,&apos;, &apos;etc&apos;, &apos;.&apos;, &apos;)&apos;, &apos;accepted&apos;, &apos;from&apos;, &apos;then&apos;, &apos;on&apos;, &apos;as&apos;, &apos;a&apos;, &apos;given&apos;, &apos;;&apos;, &apos;namely&apos;, &apos;,&apos;, &apos;the&apos;, &apos;approach&apos;, &apos;to&apos;, &apos;such&apos;, &apos;films&apos;, &apos;thenceforth&apos;, &apos;would&apos;, &apos;need&apos;, &apos;more&apos;, &apos;than&apos;, &apos;a&apos;, &apos;touch&apos;, &apos;of&apos;, &apos;irony&apos;, &apos;,&apos;, &apos;if&apos;, &apos;not&apos;, &apos;outright&apos;, &apos;comedy&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;Burning&apos;, &apos;Paradise&apos;, &apos;&quot;&apos;, &apos;put&apos;, &apos;a&apos;, &apos;stop&apos;, &apos;to&apos;, &apos;all&apos;, &apos;that&apos;, &apos;,&apos;, &apos;and&apos;, &apos;with&apos;, &apos;a&apos;, &apos;vengeance.&lt;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;It&apos;, &quot;&apos;s&quot;, &apos;not&apos;, &apos;that&apos;, &apos;there&apos;, &apos;is&apos;, &quot;n&apos;t&quot;, &apos;humor&apos;, &apos;here&apos;, &apos;;&apos;, &apos;but&apos;, &apos;it&apos;, &apos;is&apos;, &apos;a&apos;, &apos;purely&apos;, &apos;human&apos;, &apos;humor&apos;, &apos;,&apos;, &apos;as&apos;, &apos;with&apos;, &apos;the&apos;, &apos;aged&apos;, &apos;Buddhist&apos;, &apos;priest&apos;, &apos;at&apos;, &apos;the&apos;, &apos;beginning&apos;, &apos;who&apos;, &apos;somehow&apos;, &apos;manages&apos;, &apos;a&apos;, &apos;quick&apos;, &apos;feel&apos;, &apos;of&apos;, &apos;the&apos;, &apos;nubile&apos;, &apos;young&apos;, &apos;prostitute&apos;, &apos;while&apos;, &apos;hiding&apos;, &apos;in&apos;, &apos;a&apos;, &apos;bundle&apos;, &apos;of&apos;, &apos;straw&apos;, &apos;.&apos;, &apos;But&apos;, &apos;this&apos;, &apos;is&apos;, &apos;just&apos;, &apos;as&apos;, &apos;humans&apos;, &apos;are&apos;, &apos;,&apos;, &apos;not&apos;, &apos;even&apos;, &apos;Buddhist&apos;, &apos;priests&apos;, &apos;can&apos;, &apos;be&apos;, &apos;saints&apos;, &apos;all&apos;, &apos;the&apos;, &apos;time.&lt;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;When&apos;, &apos;irony&apos;, &apos;is&apos;, &apos;at&apos;, &apos;last&apos;, &apos;introduced&apos;, &apos;into&apos;, &apos;the&apos;, &apos;film&apos;, &apos;,&apos;, &apos;it&apos;, &apos;is&apos;, &apos;the&apos;, &apos;nastiest&apos;, &apos;possible&apos;, &apos;,&apos;, &apos;emanating&apos;, &apos;from&apos;, &apos;the&apos;, &quot;&apos;&quot;, &apos;abbot&apos;, &quot;&apos;&quot;, &apos;of&apos;, &apos;Red&apos;, &apos;Lotus&apos;, &apos;Temple&apos;, &apos;,&apos;, &apos;who&apos;, &apos;is&apos;, &apos;a&apos;, &apos;study&apos;, &apos;in&apos;, &apos;pure&apos;, &apos;nihilism&apos;, &apos;such&apos;, &apos;as&apos;, &apos;has&apos;, &apos;never&apos;, &apos;been&apos;, &apos;recorded&apos;, &apos;on&apos;, &apos;film&apos;, &apos;before&apos;, &apos;.&apos;, &apos;He&apos;, &apos;is&apos;, &apos;the&apos;, &apos;very&apos;, &apos;incarnation&apos;, &apos;of&apos;, &apos;Milton&apos;, &quot;&apos;s&quot;, &apos;Satan&apos;, &apos;from&apos;, &apos;&quot;&apos;, &apos;Paradise&apos;, &apos;Lost&apos;, &apos;&quot;&apos;, &apos;:&apos;, &apos;&quot;&apos;, &apos;Better&apos;, &apos;to&apos;, &apos;rule&apos;, &apos;in&apos;, &apos;Hell&apos;, &apos;than&apos;, &apos;serve&apos;, &apos;in&apos;, &apos;heaven&apos;, &apos;!&apos;, &apos;&quot;&apos;, &apos;And&apos;, &apos;if&apos;, &apos;he&apos;, &apos;ca&apos;, &quot;n&apos;t&quot;, &apos;get&apos;, &apos;to&apos;, &apos;Satan&apos;, &quot;&apos;s&quot;, &apos;hell&apos;, &apos;soon&apos;, &apos;enough&apos;, &apos;,&apos;, &apos;he&apos;, &quot;&apos;ll&quot;, &apos;turn&apos;, &apos;the&apos;, &apos;world&apos;, &apos;around&apos;, &apos;him&apos;, &apos;into&apos;, &apos;a&apos;, &apos;living&apos;, &apos;hell&apos;, &apos;he&apos;, &apos;can&apos;, &apos;rule.&lt;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;That&apos;, &quot;&apos;s&quot;, &apos;the&apos;, &apos;motif&apos;, &apos;underscoring&apos;, &apos;the&apos;, &apos;brutal&apos;, &apos;violence&apos;, &apos;of&apos;, &apos;much&apos;, &apos;of&apos;, &apos;the&apos;, &apos;imagery&apos;, &apos;here&apos;, &apos;:&apos;, &apos;It&apos;, &quot;&apos;s&quot;, &apos;not&apos;, &apos;that&apos;, &apos;the&apos;, &apos;Abbot&apos;, &apos;just&apos;, &apos;wants&apos;, &apos;to&apos;, &apos;kill&apos;, &apos;people&apos;, &apos;;&apos;, &apos;he&apos;, &apos;wants&apos;, &apos;them&apos;, &apos;to&apos;, &apos;despair&apos;, &apos;,&apos;, &apos;to&apos;, &apos;feel&apos;, &apos;utterly&apos;, &apos;hopeless&apos;, &apos;,&apos;, &apos;to&apos;, &apos;accept&apos;, &apos;his&apos;, &apos;nihilism&apos;, &apos;as&apos;, &apos;all&apos;, &apos;-&apos;, &apos;encompassing&apos;, &apos;reality&apos;, &apos;.&apos;, &apos;Thus&apos;, &apos;there&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;definite&apos;, &apos;sense&apos;, &apos;pervading&apos;, &apos;the&apos;, &apos;Red&apos;, &apos;Temple&apos;, &apos;scenes&apos;, &apos;that&apos;, &apos;there&apos;, &apos;just&apos;, &apos;might&apos;, &apos;not&apos;, &apos;be&apos;, &apos;any&apos;, &apos;other&apos;, &apos;reality&apos;, &apos;outside&apos;, &apos;of&apos;, &apos;the&apos;, &apos;Temple&apos;, &apos;itself&apos;, &apos;-&apos;, &apos;it&apos;, &apos;has&apos;, &apos;become&apos;, &apos;all&apos;, &apos;there&apos;, &apos;is&apos;, &apos;to&apos;, &apos;the&apos;, &apos;universe&apos;, &apos;,&apos;, &apos;and&apos;, &apos;the&apos;, &apos;Abbot&apos;, &apos;,&apos;, &apos;claiming&apos;, &apos;mastery&apos;, &apos;of&apos;, &apos;infinite&apos;, &apos;power&apos;, &apos;,&apos;, &apos;is&apos;, &apos;in&apos;, &apos;charge.&lt;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Of&apos;, &apos;course&apos;, &apos;,&apos;, &apos;fortunately&apos;, &apos;,&apos;, &apos;the&apos;, &apos;film&apos;, &apos;does&apos;, &quot;n&apos;t&quot;, &apos;end&apos;, &apos;there&apos;, &apos;.&apos;, &apos;Though&apos;, &apos;there&apos;, &apos;are&apos;, &apos;losses&apos;, &apos;,&apos;, &apos;the&apos;, &apos;human&apos;, &apos;will&apos;, &apos;to&apos;, &apos;be&apos;, &apos;just&apos;, &apos;ordinarily&apos;, &apos;human&apos;, &apos;at&apos;, &apos;last&apos;, &apos;prevails&apos;, &apos;.&apos;, &apos;(&apos;, &apos;If&apos;, &apos;you&apos;, &apos;want&apos;, &apos;to&apos;, &apos;know&apos;, &apos;how&apos;, &apos;,&apos;, &apos;see&apos;, &apos;the&apos;, &apos;film&apos;, &apos;!&apos;, &apos;)&apos;, &apos;Yet&apos;, &apos;there&apos;, &apos;is&apos;, &apos;no&apos;, &apos;doubt&apos;, &apos;that&apos;, &apos;,&apos;, &apos;in&apos;, &apos;viewing&apos;, &apos;this&apos;, &apos;film&apos;, &apos;,&apos;, &apos;we&apos;, &apos;visit&apos;, &apos;hell&apos;, &apos;.&apos;, &apos;Hopefully&apos;, &apos;,&apos;, &apos;we&apos;, &apos;do&apos;, &apos;not&apos;, &apos;witness&apos;, &apos;our&apos;, &apos;own&apos;, &apos;afterlives&apos;, &apos;;&apos;, &apos;but&apos;, &apos;we&apos;, &apos;certainly&apos;, &apos;feel&apos;, &apos;chastened&apos;, &apos;by&apos;, &apos;the&apos;, &apos;experience&apos;, &apos;-&apos;, &apos;and&apos;, &apos;somehow&apos;, &apos;better&apos;, &apos;for&apos;, &apos;it&apos;, &apos;over&apos;, &apos;all&apos;, &apos;.&apos;], &apos;label&apos;: &apos;pos&apos;&#125;</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第二步将训练集划分为训练集和验证集"><a class="markdownIt-Anchor" href="#第二步将训练集划分为训练集和验证集"></a> 第二步：将训练集划分为训练集和验证集</h2>
<ol>
<li>由于我们现在只有train/test这两个分类，所以我们需要创建一个新的validation set。我们可以使用.split()创建新的分类。</li>
<li>默认的数据分割是 70、30，如果我们声明split_ratio，可以改变split之间的比例，split_ratio=0.8表示80%的数据是训练集，20%是验证集。</li>
<li>我们还声明random_state这个参数，确保我们每次分割的数据集都是一样的。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认split_ratio=0.7</span></span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of validation examples: <span class="subst">&#123;len(valid_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Number of training examples: 17500</span><br><span class="line">Number of validation examples: 7500</span><br><span class="line">Number of testing examples: 25000</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第三步用训练集建立vocabulary把每个单词映射到一个数字"><a class="markdownIt-Anchor" href="#第三步用训练集建立vocabulary把每个单词映射到一个数字"></a> 第三步：用训练集建立vocabulary，把每个单词映射到一个数字。</h2>
<ul>
<li>我们使用最常见的25k个单词来构建我们的单词表，用<code>max_size</code>这个参数可以做到这一点。</li>
<li>所有其他的单词都用<code>&lt;unk&gt;</code>来表示。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从预训练的词向量（vectors）中，将当前(corpus语料库)词汇表的词向量抽取出来，构成当前 corpus 的 Vocab（词汇表）</span></span><br><span class="line"><span class="comment"># 预训练的 vectors 来自glove模型，每个单词有100维。glove模型训练的词向量参数来自很大的语料库</span></span><br><span class="line"><span class="comment"># 而我们的电影评论的语料库小一点，所以词向量需要更新，glove的词向量适合用做初始化参数。</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>, unk_init=torch.Tensor.normal_)</span><br><span class="line">LABEL.build_vocab(train_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.vector_cache/glove.6B.zip: 862MB [00:44, 19.5MB/s]                           </span><br><span class="line">100%|█████████▉| 399664/400000 [00:19&lt;00:00, 20291.20it/s]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">f'Unique tokens in TEXT vocabulary: <span class="subst">&#123;len(TEXT.vocab)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Unique tokens in LABEL vocabulary: <span class="subst">&#123;len(LABEL.vocab)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Unique tokens in TEXT vocabulary: 25002</span><br><span class="line">Unique tokens in LABEL vocabulary: 2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(LABEL.vocab.itos)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&apos;neg&apos;, &apos;pos&apos;]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(LABEL.vocab.stoi)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defaultdict(&lt;function _default_unk_index at 0x7fb5205c2a60&gt;, &#123;&apos;neg&apos;: 0, &apos;pos&apos;: 1&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(TEXT.vocab.stoi)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defaultdict(&lt;function _default_unk_index at 0x7fb5205c2a60&gt;, &#123;&apos;&lt;unk&gt;&apos;: 0, &apos;&lt;pad&gt;&apos;: 1, &apos;the&apos;: 2, &apos;,&apos;: 3, &apos;.&apos;: 4, &apos;and&apos;: 5, &apos;a&apos;: 6, &apos;of&apos;: 7, &apos;to&apos;: 8, &apos;is&apos;: 9, &apos;in&apos;: 10, &apos;I&apos;: 11, &apos;it&apos;: 12, &apos;that&apos;: 13, &apos;&quot;&apos;: 14, &quot;&apos;s&quot;: 15, &apos;this&apos;: 16, &apos;-&apos;: 17, &apos;/&gt;&lt;br&apos;: 18, &apos;was&apos;: 19, &apos;as&apos;: 20, &apos;with&apos;: 21, &apos;movie&apos;: 22, &apos;for&apos;: 23, &apos;film&apos;: 24, &apos;The&apos;: 25, &apos;but&apos;: 26, &apos;(&apos;: 27, &apos;)&apos;: 28, &quot;n&apos;t&quot;: 29, &apos;on&apos;: 30, &apos;you&apos;: 31, &apos;are&apos;: 32, &apos;not&apos;: 33, &apos;have&apos;: 34, &apos;his&apos;: 35, &apos;be&apos;: 36, &apos;he&apos;: 37, &apos;one&apos;: 38, &apos;at&apos;: 39, &apos;by&apos;: 40, &apos;all&apos;: 41, &apos;!&apos;: 42, &apos;an&apos;: 43, &apos;who&apos;: 44, &apos;they&apos;: 45, &apos;from&apos;: 46, &apos;like&apos;: 47, &apos;so&apos;: 48, &apos;her&apos;: 49, &quot;&apos;&quot;: 50, &apos;about&apos;: 51, &apos;or&apos;: 52, &apos;has&apos;: 53, &apos;It&apos;: 54, &apos;out&apos;: 55, &apos;just&apos;: 56, &apos;do&apos;: 57, &apos;?&apos;: 58, &apos;some&apos;: 59, &apos;good&apos;: 60, &apos;more&apos;: 61, &apos;very&apos;: 62, &apos;would&apos;: 63, &apos;up&apos;: 64, &apos;what&apos;: 65, &apos;This&apos;: 66, &apos;there&apos;: 67, &apos;time&apos;: 68, &apos;can&apos;: 69, &apos;when&apos;: 70, &apos;which&apos;: 71, &apos;had&apos;: 72, &apos;she&apos;: 73, &apos;if&apos;: 74, &apos;only&apos;: 75, &apos;story&apos;: 76, &apos;really&apos;: 77, &apos;were&apos;: 78, &apos;their&apos;: 79, &apos;see&apos;: 80, &apos;no&apos;: 81, &apos;even&apos;: 82, &apos;my&apos;: 83, &apos;me&apos;: 84, &apos;did&apos;: 85, &apos;does&apos;: 86, &apos;...&apos;: 87, &apos;than&apos;: 88, &apos;:&apos;: 89, &apos;much&apos;: 90, &apos;could&apos;: 91, &apos;been&apos;: 92, &apos;get&apos;: 93, &apos;into&apos;: 94, &apos;we&apos;: 95, &apos;well&apos;: 96, &apos;bad&apos;: 97, &apos;people&apos;: 98, &apos;will&apos;: 99, &apos;because&apos;: 100, ......,&apos;chieftain&apos;: 24995, &apos;child.&lt;br&apos;: 24996, &apos;childbirth&apos;: 24997, &apos;chilly&apos;: 24998, &apos;chime&apos;: 24999, &apos;chinese&apos;: 25000, &apos;chokes&apos;: 25001&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 语料库单词频率越高，索引越靠前。前两个默认为unk和pad。</span></span><br><span class="line">print(TEXT.vocab.itos)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;and&apos;, &apos;a&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;, &apos;in&apos;, &apos;I&apos;, &apos;it&apos;, &apos;that&apos;, &apos;&quot;&apos;, &quot;&apos;s&quot;, &apos;this&apos;, &apos;-&apos;, &apos;/&gt;&lt;br&apos;, &apos;was&apos;, &apos;as&apos;, &apos;with&apos;, &apos;movie&apos;, &apos;for&apos;, &apos;film&apos;, &apos;The&apos;, &apos;but&apos;, &apos;(&apos;, &apos;)&apos;, &quot;n&apos;t&quot;, &apos;on&apos;, &apos;you&apos;, &apos;are&apos;, &apos;not&apos;, &apos;have&apos;, &apos;his&apos;, &apos;be&apos;, &apos;he&apos;, &apos;one&apos;, &apos;at&apos;, &apos;by&apos;, &apos;all&apos;, &apos;!&apos;, &apos;an&apos;, &apos;who&apos;, &apos;they&apos;, &apos;from&apos;, &apos;like&apos;, &apos;so&apos;, &apos;her&apos;, &quot;&apos;&quot;, &apos;about&apos;, &apos;or&apos;, &apos;has&apos;, &apos;It&apos;, &apos;out&apos;, &apos;just&apos;, &apos;do&apos;, &apos;?&apos;, &apos;some&apos;, &apos;good&apos;, &apos;more&apos;, &apos;very&apos;, &apos;would&apos;, &apos;up&apos;, &apos;what&apos;, &apos;This&apos;, &apos;there&apos;, &apos;time&apos;, &apos;can&apos;, &apos;when&apos;, &apos;which&apos;, &apos;had&apos;, &apos;she&apos;, &apos;if&apos;, &apos;only&apos;, &apos;story&apos;,....... ]</span><br></pre></td></tr></table></figure>
<p>25002多出来的2就是<code>&lt;unk&gt;</code>和<code>&lt;pad&gt;</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(TEXT.vocab.freqs.most_common(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;the&apos;, 200806), (&apos;,&apos;, 190507), (&apos;.&apos;, 163859), (&apos;and&apos;, 108678), (&apos;a&apos;, 108379), (&apos;of&apos;, 99904), (&apos;to&apos;, 92850), (&apos;is&apos;, 75910), (&apos;in&apos;, 60829), (&apos;I&apos;, 54227), (&apos;it&apos;, 53199), (&apos;that&apos;, 48835), (&apos;&quot;&apos;, 43297), (&quot;&apos;s&quot;, 42758), (&apos;this&apos;, 41960), (&apos;-&apos;, 36735), (&apos;/&gt;&lt;br&apos;, 35706), (&apos;was&apos;, 34813), (&apos;as&apos;, 29872), (&apos;with&apos;, 29443)]</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第四步创建iterators每个iteration都会返回一个batch的样本"><a class="markdownIt-Anchor" href="#第四步创建iterators每个iteration都会返回一个batch的样本"></a> 第四步：创建iterators，每个iteration都会返回一个batch的样本。</h2>
<ul>
<li>我们会使用<code>BucketIterator</code>。<code>BucketIterator</code>会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。</li>
<li>严格来说，我们这份notebook中的模型代码都有一个问题，也就是我们把<code>&lt;pad&gt;</code>也当做了模型的输入进行训练。更好的做法是在模型中把由<code>&lt;pad&gt;</code>产生的输出给消除掉。在这这里我们简单处理，直接把<code>&lt;pad&gt;</code>也用作模型输入了。由于<code>&lt;pad&gt;</code>数量不多，模型的效果也不差。</li>
<li>如果我们有GPU，还可以指定每个iteration返回的tensor都在GPU上。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相当于把样本划分batch，知识多做了一步，把相等长度的单词尽可能的划分到一个batch，不够长的就用padding。</span></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data),</span><br><span class="line">    batch_size = BATCH_SIZE,</span><br><span class="line">    device = device</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">next(iter(train_iterator)).label</span><br><span class="line">next(iter(train_iterator)).text</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[  66,  603, 2228,  ..., 1863,    0,   66],</span><br><span class="line">        [  22,  533,    3,  ...,    2,    9,    9],</span><br><span class="line">        [  19,  119,  106,  ..., 1449,   33,    6],</span><br><span class="line">        ...,</span><br><span class="line">        [   1,    1,    1,  ...,    1,    1,    1],</span><br><span class="line">        [   1,    1,    1,  ...,    1,    1,    1],</span><br><span class="line">        [   1,    1,    1,  ...,    1,    1,    1]], device=&apos;cuda:0&apos;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多运行一次可以发现一条评论的单词长度会变</span></span><br><span class="line">next(iter(train_iterator))</span><br><span class="line">next(iter(train_iterator)).text</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[2458,   11,    0,  ...,   11,  171, 9535],</span><br><span class="line">        [   6,   34, 4148,  ...,   34,   31, 1697],</span><br><span class="line">        [ 403,   92, 1139,  ...,  124,  213,  133],</span><br><span class="line">        ...,</span><br><span class="line">        [   1,    1,    1,  ...,    1,    1,    1],</span><br><span class="line">        [   1,    1,    1,  ...,    1,    1,    1],</span><br><span class="line">        [   1,    1,    1,  ...,    1,    1,    1]], device=&apos;cuda:0&apos;)</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第五步创建-word-averaging-模型"><a class="markdownIt-Anchor" href="#第五步创建-word-averaging-模型"></a> 第五步：创建 Word Averaging 模型</h2>
<ul>
<li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li>
<li>我们使用<a href="https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.functional.avg_pool2d" target="_blank" rel="noopener"><code>avg_pool2d</code></a>来做average pooling。我们的目标是把sentence length那个维度平均成1，然后保留embedding这个维度。</li>
<li><code>avg_pool2d</code>的kernel size是 (<code>embedded.shape[1]</code>, 1)，所以句子长度的那个维度会被压扁。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    super().__init__()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># embedding的作用就是将每个单词变成一个词向量</span></span><br><span class="line">    <span class="comment"># vocab_size=词汇表长度，embedding_dim=每个单词的维度</span></span><br><span class="line">    <span class="comment"># padding_idx：如果提供的话，输出遇到此下标时用零填充。这里如果遇到padding的单词就用0填充。</span></span><br><span class="line">    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output_dim输出的维度，一个数就可以了，=1</span></span><br><span class="line">    self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span>  <span class="comment"># text维度为(sent_len, 1)</span></span><br><span class="line">    embedded = self.embedding(text)</span><br><span class="line">    <span class="comment"># text 下面会指定，为一个batch的数据</span></span><br><span class="line">    <span class="comment"># embedded = [sent_len, batch_size, emb_dim]</span></span><br><span class="line">    <span class="comment"># sen_len 一条评论的单词数</span></span><br><span class="line">    <span class="comment"># batch_size 一个batch有多少条评论</span></span><br><span class="line">    <span class="comment"># emb_dim 一个单词的维度</span></span><br><span class="line">    <span class="comment"># 假设[sent_len, batch_size, emb_dim] = (1000, 64, 100)</span></span><br><span class="line">    <span class="comment"># 则进行运算: (text: 1000, 64, 25000)*(self.embedding: 1000, 25000, 100) = (1000, 64, 100)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># [batch_size, sent_len, emb_dim] 更换顺序</span></span><br><span class="line">    embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># [batch_size, embedding_dim]把单词长度的维度压扁为1，并降维</span></span><br><span class="line">    <span class="comment"># embedded 为input_size，(embedded.shape[1], 1)) 为kernel_size</span></span><br><span class="line">    <span class="comment"># squeeze(1)表示删除索引为1的那个维度</span></span><br><span class="line">    pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># (batch_size, embedding_dim)*(embedding_dim, output_dim) = (batch_size, output_dim)</span></span><br><span class="line">    <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)  <span class="comment"># 25002</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PAD_IDX = 1 为pad的索引</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(PAD_IDX)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计参数数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></span><br><span class="line">  <span class="comment"># numel()函数：返回数组中元素的个数</span></span><br><span class="line">  <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The model has 2,500,301 trainable parameters</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第六步初始化参数"><a class="markdownIt-Anchor" href="#第六步初始化参数"></a> 第六步：初始化参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 把上面vectors="glove.6B.100d"取出的词向量作为初始化参数</span></span><br><span class="line"><span class="comment"># 数量为25000*100个参数，25000个单词，每个单词的词向量维度为100</span></span><br><span class="line">pretrained_embeddings = TEXT.vocab.vectors</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],</span><br><span class="line">        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],</span><br><span class="line">        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.2455, -0.0385, -0.4767,  ..., -0.2939, -0.0752,  0.0441],</span><br><span class="line">        [ 0.4327,  0.3958,  0.5878,  ..., -1.1461,  0.2348, -0.2359],</span><br><span class="line">        [-0.3970,  0.4024,  1.0612,  ..., -0.0136, -0.3363,  0.6442]],</span><br><span class="line">       device=&apos;cuda:0&apos;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]  <span class="comment"># UNK_IDX = 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词汇表25002个单词，前两个unk和pad也需要初始化，把它们初始化为0</span></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.embedding.weight.data</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.2455, -0.0385, -0.4767,  ..., -0.2939, -0.0752,  0.0441],</span><br><span class="line">        [ 0.4327,  0.3958,  0.5878,  ..., -1.1461,  0.2348, -0.2359],</span><br><span class="line">        [-0.3970,  0.4024,  1.0612,  ..., -0.0136, -0.3363,  0.6442]])</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第七步训练模型"><a class="markdownIt-Anchor" href="#第七步训练模型"></a> 第七步：训练模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，这个BCEWithLogitsLoss特殊情况，二分类损失函数</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 送到GPU上去</span></span><br><span class="line">model = model.to(device)</span><br><span class="line">criterion = criterion.to(device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算预测的准确率</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># .round函数 四舍五入，rounded_preds要么为0，要么为1</span></span><br><span class="line">  <span class="comment"># neg为0, pos为1</span></span><br><span class="line">  rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># convert into float for division</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  a = torch.tensor([1, 1])</span></span><br><span class="line"><span class="string">  b = torch.tensor([1, 1])</span></span><br><span class="line"><span class="string">  print(a == b)</span></span><br><span class="line"><span class="string">  output: tensor([1, 1], dtype=torch.uint8)</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  a = torch.tensor([1, 0])</span></span><br><span class="line"><span class="string">  b = torch.tensor([1, 1])</span></span><br><span class="line"><span class="string">  print(a == b)</span></span><br><span class="line"><span class="string">  output: tensor([1, 0], dtype=torch.uint8)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  correct = (rounded_preds == y).float()</span><br><span class="line">  acc = correct.sum() / len(correct)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">  </span><br><span class="line">  epoch_loss = <span class="number">0</span></span><br><span class="line">  epoch_acc = <span class="number">0</span></span><br><span class="line">  total_len = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># model.train()代表了训练模式</span></span><br><span class="line">  <span class="comment"># model.train() ：启用 BatchNormalization 和 Dropout</span></span><br><span class="line">  <span class="comment"># model.eval() ：不启用 BatchNormalization 和 Dropout</span></span><br><span class="line">  model.train() </span><br><span class="line">  </span><br><span class="line">  <span class="comment"># iterator为train_iterator</span></span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">    <span class="comment"># 梯度清零，加这步防止梯度叠加</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># batch.text 就是上面forward函数的参数text</span></span><br><span class="line">    <span class="comment"># 压缩维度，不然跟 batch.label 维度对不上</span></span><br><span class="line">    predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    loss = criterion(predictions, batch.label)</span><br><span class="line">    acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">    </span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step() <span class="comment"># 梯度下降</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loss.item() 以及本身除以了 len(batch.label)</span></span><br><span class="line">    <span class="comment"># 所以得再乘一次，得到一个batch的损失，累加得到所有样本损失</span></span><br><span class="line">    epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># (acc.item(): 一个batch的正确率) * batch数 = 正确数</span></span><br><span class="line">    <span class="comment"># train_iterator 所有batch的正确数累加</span></span><br><span class="line">    epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算 train_iterator 所有样本的数量，应该是17500</span></span><br><span class="line">    total_len += len(batch.label)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># epoch_loss / total_len ：train_iterator所有batch的损失</span></span><br><span class="line">  <span class="comment"># epoch_acc / total_len ：train_iterator所有batch的正确率</span></span><br><span class="line">  <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不用优化器了</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">  </span><br><span class="line">  epoch_loss = <span class="number">0</span></span><br><span class="line">  epoch_acc = <span class="number">0</span></span><br><span class="line">  total_len = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 转成测试模式，冻结dropout层或其他层</span></span><br><span class="line">  model.eval() </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># iterator为valid_iterator</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># 没有反向传播和梯度下降</span></span><br><span class="line">      </span><br><span class="line">      predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">      loss = criterion(predictions, batch.label)</span><br><span class="line">      acc = binary_accuracy(predictions, batch.label)</span><br><span class="line"></span><br><span class="line">      epoch_loss += loss.item() * len(batch.label)</span><br><span class="line">      epoch_acc += acc.item() * len(batch.label)</span><br><span class="line">      total_len += len(batch.label)</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 调回训练模式</span></span><br><span class="line">  model.train()</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> epoch_loss / total_len, epoch_acc / total_len</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看每个epoch的时间</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span>  </span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第八步查看模型运行结果"><a class="markdownIt-Anchor" href="#第八步查看模型运行结果"></a> 第八步：查看模型运行结果</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)  <span class="comment"># 初试的验证集loss设置为无穷大</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">  start_time = time.time()</span><br><span class="line">  </span><br><span class="line">  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">  </span><br><span class="line">  end_time = time.time()</span><br><span class="line">  </span><br><span class="line">  epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 只要模型效果变好，就存模型(参数)</span></span><br><span class="line">  <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">    best_valid_loss = valid_loss</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">  print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">  print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">  print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> | Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.577 | Train Acc: 71.55%</span><br><span class="line">	 Val. Loss: 0.461 | Val. Acc: 80.96%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.467 | Train Acc: 83.26%</span><br><span class="line">	 Val. Loss: 0.375 | Val. Acc: 85.43%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.402 | Train Acc: 87.45%</span><br><span class="line">	 Val. Loss: 0.350 | Val. Acc: 87.33%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.359 | Train Acc: 89.32%</span><br><span class="line">	 Val. Loss: 0.356 | Val. Acc: 88.07%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.327 | Train Acc: 90.38%</span><br><span class="line">	 Val. Loss: 0.361 | Val. Acc: 88.72%</span><br><span class="line">Epoch: 06 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.298 | Train Acc: 91.18%</span><br><span class="line">	 Val. Loss: 0.373 | Val. Acc: 89.03%</span><br><span class="line">Epoch: 07 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.274 | Train Acc: 92.34%</span><br><span class="line">	 Val. Loss: 0.382 | Val. Acc: 89.37%</span><br><span class="line">Epoch: 08 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.253 | Train Acc: 92.87%</span><br><span class="line">	 Val. Loss: 0.395 | Val. Acc: 89.49%</span><br><span class="line">Epoch: 09 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.235 | Train Acc: 93.58%</span><br><span class="line">	 Val. Loss: 0.410 | Val. Acc: 89.61%</span><br><span class="line">Epoch: 10 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.220 | Train Acc: 94.21%</span><br><span class="line">	 Val. Loss: 0.421 | Val. Acc: 89.75%</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="第九步预测结果"><a class="markdownIt-Anchor" href="#第九步预测结果"></a> 第九步：预测结果</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用保存的模型参数预测数据</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">"wordavg-model.pt"</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># spacy是分词工具，跟NLTK类似</span></span><br><span class="line"><span class="keyword">import</span> spacy  </span><br><span class="line">nlp = spacy.load(<span class="string">'en'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(sentence)</span>:</span></span><br><span class="line">  <span class="comment"># 分词</span></span><br><span class="line">  tokenized = [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> nlp.tokenizer(sentence)]</span><br><span class="line">  <span class="comment"># sentence 的索引</span></span><br><span class="line">  indexed = [TEXT.vocab.stoi[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokenized]</span><br><span class="line">  </span><br><span class="line">  tensor = torch.LongTensor(indexed).to(device)  <span class="comment"># seq_len</span></span><br><span class="line">  tensor = tensor.unsqueeze(<span class="number">1</span>)   <span class="comment"># seq_len * batch_size (1)</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># tensor与text一样的tensor</span></span><br><span class="line">  prediction = torch.sigmoid(model(tensor))</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> prediction.item()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"I love this film bad"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">9.618100193620194e-06</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"This film is great"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test Loss: 0.391 | Test Acc: 86.04%</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="rnn模型bilstm"><a class="markdownIt-Anchor" href="#rnn模型bilstm"></a> RNN模型（BiLSTM）</h2>
<ul>
<li>下面我们尝试把模型换成一个<strong>recurrent neural network</strong> (RNN)。RNN经常会被用来encode一个sequence<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mtext>RNN</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = \text{RNN}(x_t, h_{t-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">RNN</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
</li>
<li>我们使用最后一个hidden state <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">h_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>来表示整个句子。</li>
<li>然后我们把<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">h_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>通过一个线性变换<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>，然后用来预测句子的情感。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, output_dim,</span></span></span><br><span class="line"><span class="function"><span class="params">               n_layers, bidirectional, dropout, pad_idx)</span>:</span></span><br><span class="line">    </span><br><span class="line">    super().__init__()</span><br><span class="line">    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># embedding_dim: 每个词向量的维度</span></span><br><span class="line">    <span class="comment"># hidden_dim: 隐藏层的维度</span></span><br><span class="line">    <span class="comment"># num_layers: 神经网络深度，纵向深度</span></span><br><span class="line">    <span class="comment"># bidrectional: 是否双向循环RNN</span></span><br><span class="line">    <span class="comment"># dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。</span></span><br><span class="line">    <span class="comment"># 经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。</span></span><br><span class="line">    self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,</span><br><span class="line">                       bidirectional=bidirectional, dropout=dropout)</span><br><span class="line">    </span><br><span class="line">    self.fc = nn.Linear(hidden_dim*<span class="number">2</span>, output_dim)  <span class="comment"># *2是因为BiLSTM</span></span><br><span class="line">    self.dropout = nn.Dropout(dropout)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    embedded = self.dropout(self.embedding(text)) <span class="comment"># [sent len, batch size, emb dim]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output = [sent len, batch size, hid dim * num directions]</span></span><br><span class="line">    <span class="comment"># hidden = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">    <span class="comment"># cell = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">    output, (hidden, cell) = self.rnn(embedded)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers</span></span><br><span class="line">    <span class="comment"># and apply dropout</span></span><br><span class="line">    <span class="comment"># [batch size, hid dim * num directions], 横着拼接的</span></span><br><span class="line">    <span class="comment"># 倒数第一个和倒数第二个是BiLSTM最后要保留的状态</span></span><br><span class="line">    hidden = self.dropout(torch.cat((hidden[<span class="number">-2</span>, :, :], hidden[<span class="number">-1</span>, :, :]), dim=<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.fc(hidden.squeeze)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)  <span class="comment"># 25002</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PAD_IDX = 1 为pad的索引</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,</span><br><span class="line">            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The model has 4,810,857 trainable parameters</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="初始化参数"><a class="markdownIt-Anchor" href="#初始化参数"></a> 初始化参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pretrained_embeddings = TEXT.vocab.vectors</span></span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line"></span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]  <span class="comment"># UNK_IDX = 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词汇表25002个单词，前两个unk和pad也需要初始化，把它们初始化为0</span></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">print(model.embedding.weight.data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],</span><br><span class="line">        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.2455, -0.0385, -0.4767,  ..., -0.2939, -0.0752,  0.0441],</span><br><span class="line">        [ 0.4327,  0.3958,  0.5878,  ..., -1.1461,  0.2348, -0.2359],</span><br><span class="line">        [-0.3970,  0.4024,  1.0612,  ..., -0.0136, -0.3363,  0.6442]],</span><br><span class="line">       device=&apos;cuda:0&apos;)</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="训练rnn模型"><a class="markdownIt-Anchor" href="#训练rnn模型"></a> 训练RNN模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import torch.optim as optim</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，这个BCEWithLogitsLoss特殊情况，二分类损失函数</span></span><br><span class="line"><span class="comment"># criterion = nn.BCEWithLogitsLoss()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 送到GPU上去</span></span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># criterion = criterion.to(device)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'lstm-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.594 | Train Acc: 70.69%</span><br><span class="line">	 Val. Loss: 0.493 |  Val. Acc: 78.31%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.502 | Train Acc: 82.04%</span><br><span class="line">	 Val. Loss: 0.373 |  Val. Acc: 84.53%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.440 | Train Acc: 85.97%</span><br><span class="line">	 Val. Loss: 0.363 |  Val. Acc: 86.17%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.391 | Train Acc: 88.27%</span><br><span class="line">	 Val. Loss: 0.342 |  Val. Acc: 87.73%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.357 | Train Acc: 89.50%</span><br><span class="line">	 Val. Loss: 0.350 |  Val. Acc: 88.20%</span><br><span class="line">Epoch: 06 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.325 | Train Acc: 90.49%</span><br><span class="line">	 Val. Loss: 0.360 |  Val. Acc: 88.60%</span><br><span class="line">Epoch: 07 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.299 | Train Acc: 91.28%</span><br><span class="line">	 Val. Loss: 0.376 |  Val. Acc: 88.88%</span><br><span class="line">Epoch: 08 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.276 | Train Acc: 92.13%</span><br><span class="line">	 Val. Loss: 0.382 |  Val. Acc: 89.25%</span><br><span class="line">Epoch: 09 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.256 | Train Acc: 92.91%</span><br><span class="line">	 Val. Loss: 0.396 |  Val. Acc: 89.45%</span><br><span class="line">Epoch: 10 | Epoch Time: 0m 6s</span><br><span class="line">	Train Loss: 0.239 | Train Acc: 93.41%</span><br><span class="line">	 Val. Loss: 0.410 |  Val. Acc: 89.60%</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="预测结果"><a class="markdownIt-Anchor" href="#预测结果"></a> 预测结果</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'lstm-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test Loss: 0.381 | Test Acc: 86.31%</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="cnn模型"><a class="markdownIt-Anchor" href="#cnn模型"></a> CNN模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, n_filters, filter_sizes,</span></span></span><br><span class="line"><span class="function"><span class="params">               output_dim, dropout, pad_idx)</span>:</span></span><br><span class="line">    super().__init__()</span><br><span class="line">    </span><br><span class="line">    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">    self.convs = nn.ModuleList([</span><br><span class="line">        nn.Conv2d(in_channels = <span class="number">1</span>, out_channels = n_filters,</span><br><span class="line">                  kernel_size = (fs, embedding_dim))</span><br><span class="line">        <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">    self.dropout = nn.Dropout(dropout)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    text = text.permute(<span class="number">1</span>, <span class="number">0</span>)        <span class="comment"># [batch size, sent len]</span></span><br><span class="line">    embedded = self.embedding(text)  <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">    embedded = embedded.unsqueeze(<span class="number">1</span>) <span class="comment"># [batch size, 1, sent len, emb dim]</span></span><br><span class="line">    conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># conv_n = [batch size, n_filters, sent len - filter_sizes[n]]</span></span><br><span class="line">    </span><br><span class="line">    pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># pooled_n = [batch size, n_filters]</span></span><br><span class="line">    </span><br><span class="line">    cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">N_FILTERS = <span class="number">100</span></span><br><span class="line">FILTER_SIZES = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line"></span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'CNN-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch: 01 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.653 | Train Acc: 61.07%</span><br><span class="line">	 Val. Loss: 0.504 |  Val. Acc: 78.20%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.427 | Train Acc: 80.60%</span><br><span class="line">	 Val. Loss: 0.352 |  Val. Acc: 84.85%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.306 | Train Acc: 87.18%</span><br><span class="line">	 Val. Loss: 0.315 |  Val. Acc: 86.56%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.221 | Train Acc: 91.37%</span><br><span class="line">	 Val. Loss: 0.303 |  Val. Acc: 87.43%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 31s</span><br><span class="line">	Train Loss: 0.161 | Train Acc: 93.86%</span><br><span class="line">	 Val. Loss: 0.319 |  Val. Acc: 87.47%</span><br><span class="line">Epoch: 06 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.114 | Train Acc: 95.86%</span><br><span class="line">	 Val. Loss: 0.347 |  Val. Acc: 87.21%</span><br><span class="line">Epoch: 07 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.078 | Train Acc: 97.43%</span><br><span class="line">	 Val. Loss: 0.355 |  Val. Acc: 87.41%</span><br><span class="line">Epoch: 08 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.055 | Train Acc: 98.30%</span><br><span class="line">	 Val. Loss: 0.386 |  Val. Acc: 87.33%</span><br><span class="line">Epoch: 09 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.041 | Train Acc: 98.85%</span><br><span class="line">	 Val. Loss: 0.412 |  Val. Acc: 87.48%</span><br><span class="line">Epoch: 10 | Epoch Time: 0m 30s</span><br><span class="line">	Train Loss: 0.031 | Train Acc: 99.08%</span><br><span class="line">	 Val. Loss: 0.440 |  Val. Acc: 87.23%</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'CNN-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Test Loss: 0.334 | Test Acc: 85.49%</span><br></pre></td></tr></table></figure>
<p> </p>
<hr>
<h2 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h2>
<ol>
<li><a href="https://github.com/bentrevett/pytorch-sentiment-analysis" target="_blank" rel="noopener">https://github.com/bentrevett/pytorch-sentiment-analysis</a></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Chenhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://chenhao.space/post/a5b86241.html">http://chenhao.space/post/a5b86241.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://chenhao.space">Chenhao's Studio</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SA/">SA</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/post/f4e09fe1.html"><i class="fa fa-chevron-left">  </i><span>深度学习之CNN笔记</span></a></div><div class="next-post pull-right"><a href="/post/fc895aa3.html"><span>Pytorch-MLP</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=27867140&auto=0&height=66"></iframe></div><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'GjEQNoax0r5e528SPYqxOPME-gzGzoHsz',
  appKey:'vLtYMpgjD90htoYal5mIjk4b',
  placeholder:'Just go go',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(https://chenhao-1300052108.cos.ap-beijing.myqcloud.com/ch-Pic/Hexo%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3%E5%9B%BE%E7%89%87/intro/bcar-bg.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By Chenhao</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">🍉🍉🍉 Hi, welcome to my <a href="http://chenhao.space">blog</a>! 🍉🍉🍉</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/love.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>